# Logistic回归

由于团队对Logistic回归的理解还不够深刻，因此下面有请小园同志解释一下什么是Logistic回归。

大家好，我是小园，下面我给大家讲一下什么是Logistic回归。首先我们来认识一下sigmoid函数。

## sigmoid函数

sigmoid函数的具体计算公式如下：

![1](http://chart.googleapis.com/chart?cht=tx&chl=%5Csigma(x)%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-x%7D%7D)

sigmoid函数的图像如下：

![2](https://github.com/im-iron-man/data-analysis/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/6/image/1.png)

其中sigmoid函数有如下几个特点：

- 介于0，1之间：当x趋于负无穷时，y趋近于0；当x趋于正无穷时，y趋近于1
- 关于(0，0.5)对称：sigmoid函数是一个中心对称的函数

## Logistic回归

虽然Logistic回归以回归二字结尾，但是它确确实实是一个分类算法。那为什么它会以回归二字结尾呢？这是因为Logistic回归本质上把一个分类问题看成了一个回归问题，即把目标变量看作连续的，以此训练出sigmoid函数，然后再以0.5作为阈值进行分类。下面我就讲一下Logistic回归的具体步骤。

### 回归

设![3](http://chart.googleapis.com/chart?cht=tx&chl=(x%5E%7B(i)%7D%2Cy%5E%7B(i)%7D)_%7B1%5Cleq%20i%5Cleq%20n%7D)为样本，其中目标变量的值为0或1。假设目标变量与特征满足如下关系：

![4](http://chart.googleapis.com/chart?cht=tx&chl=f(x)%3D%5Csigma(%5Ctheta_0%2B%5Ctheta_1%20x))

然后将样本带入，得到误差函数

![5](http://chart.googleapis.com/chart?cht=tx&chl=h(%5Ctheta_0%2C%5Ctheta_1)%3D%5Csum%5Climits_%7Bi%3D1%7D%5En(f(x%5E%7B(i)%7D)-y%5E%7B(i)%7D)%5E2)

然后极小化这个关于![6](http://chart.googleapis.com/chart?cht=tx&chl=%5Ctheta_0)和![7](http://chart.googleapis.com/chart?cht=tx&chl=%5Ctheta_1)的二元函数，得到![8](http://chart.googleapis.com/chart?cht=tx&chl=%5Chat%7B%5Ctheta_0%7D)和![9](http://chart.googleapis.com/chart?cht=tx&chl=%5Chat%7B%5Ctheta_1%7D)。

## 习题集