# Logistic回归

由于团队对Logistic回归的理解还不够深刻，因此下面有请小园同志解释一下什么是Logistic回归。

大家好，我是小园，下面我给大家讲一下什么是Logistic回归。首先我们来认识一下sigmoid函数。

## sigmoid函数

sigmoid函数的具体计算公式如下：

![1](http://chart.googleapis.com/chart?cht=tx&chl=%5Csigma(x)%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-x%7D%7D)

sigmoid函数的图像如下：

![2](https://github.com/im-iron-man/data-analysis/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/6/image/1.png)

其中sigmoid函数有如下几个特点：

- 介于0，1之间：当x趋于负无穷时，y趋近于0；当x趋于正无穷时，y趋近于1
- 关于(0，0.5)对称：sigmoid函数是一个中心对称的函数

## Logistic回归

虽然Logistic回归以回归二字结尾，但是它确确实实是一个分类算法。那为什么它会以回归二字结尾呢？这是因为Logistic回归本质上把一个分类问题看成了一个回归问题，即把目标变量看作连续的，以此训练出sigmoid函数，然后再以0.5作为阈值进行分类。下面我就讲一下Logistic回归的具体步骤。

### 回归

设![3](http://chart.googleapis.com/chart?cht=tx&chl=(x%5E%7B(i)%7D%2Cy%5E%7B(i)%7D)_%7B1%5Cleq%20i%5Cleq%20n%7D)为样本，其中目标变量的值为0或1。假设目标变量与特征满足关系![4](http://chart.googleapis.com/chart?cht=tx&chl=f(x)%3D%5Csigma(%5Ctheta_0%2B%5Ctheta_1%20x))。

再将样本带入，得到误差函数![5](http://chart.googleapis.com/chart?cht=tx&chl=h(%5Ctheta_0%2C%5Ctheta_1)%3D%5Csum%5Climits_%7Bi%3D1%7D%5En(f(x%5E%7B(i)%7D)-y%5E%7B(i)%7D)%5E2)。

然后极小化这个关于![6](http://chart.googleapis.com/chart?cht=tx&chl=%5Ctheta_0)和![7](http://chart.googleapis.com/chart?cht=tx&chl=%5Ctheta_1)的二元函数，得到![8](http://chart.googleapis.com/chart?cht=tx&chl=%5Chat%7B%5Ctheta_0%7D)和![9](http://chart.googleapis.com/chart?cht=tx&chl=%5Chat%7B%5Ctheta_1%7D)。下面我们就用得到的函数![10](http://chart.googleapis.com/chart?cht=tx&chl=%5Chat%7Bf%7D(x)%3D%5Csigma(%5Chat%7B%5Ctheta_0%7D%2B%5Chat%7B%5Ctheta_1%7Dx))来预测特征![11](http://chart.googleapis.com/chart?cht=tx&chl=x_0)所在的类：

- 当![12](http://chart.googleapis.com/chart?cht=tx&chl=%5Chat%7Bf%7D(x_0)%3E0.5)时，我们将它分在1类
- 当![13](http://chart.googleapis.com/chart?cht=tx&chl=%5Chat%7Bf%7D(x_0)%3C0.5)时，我们将它分在0类

### 例子

下面以鸢尾花数据集为例，查看Logistic回归的分类结果：

```python
# -*- coding: utf-8 -*-
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn import linear_model, datasets

# 步骤一：明确分类器
clf = linear_model.LogisticRegression()

# 步骤二：训练
iris = datasets.load_iris()
X = iris.data[:100, :2]
y = iris.target[:100]
clf.fit(X, y)

# 步骤三：预测
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, .02), np.arange(y_min, y_max, .02))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
ZZ = Z.reshape(xx.shape)

# 步骤四：绘图
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
plt.figure()
plt.pcolormesh(xx, yy, ZZ, cmap=cmap_light)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.show()
```

### 线性分类器

从上述例子可以看出，Logistic回归有条很明显的分类边界。由于这条分类边界是一条直线，因此Logistic回归是一个线性分类器。由于分类边界满足：![14](http://chart.googleapis.com/chart?cht=tx&chl=%5Chat%7Bf%7D(x)%3D0.5)，即直线方程为![15](http://chart.googleapis.com/chart?cht=tx&chl=%5Ctheta_0%2B%5Ctheta_1x%3D0)。

## 习题集

### 手写体数字识别

请以Logitstic回归作为分类器，写出手写体数字识别的代码。

### 预测马的存活率


```
                   .-' _..`.    虽然我们团队没有医生，更没有动物学专家，但是
                  /  .'_.'.'    我们希望能通过马当前的状态，用数据来预测马的
                 | .' (.)`.     存活率，以此及时得为病马提供治疗。
                 ;'   ,_   `.   
 .--.__________.'    ;  `.;-'   数据来自http://archive.ics.uci.edu/ml/datasets/Horse+Colic。
|  ./               /           我们已将数据存放在https://github.com/im-iron-man/data-analysis/tree/master/机器学习/6/horse，
|  |               /            其中horseColicTraining.txt为训练集，horseColicTest.txt为测试集。
`..'`-._  _____, ..'            数据共有21个特征表示马的21个体征，且有一个可能为0，可能为1的目标变量表示马的存活情况。
     / | |     | |\ \           
    / /| |     | | \ \          下面请使用Logistic回归分类，要求如下：
   / / | |     | |  \ \         - 使用horseColicTraining.txt为训练集，训练分类器
  /_/  |_|     |_|   \_\        - 使用horseColicTest.txt为测试集，最终计算出错误率
 |__\  |__\    |__\  |__\ 
```

